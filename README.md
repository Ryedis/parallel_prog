# Отчет лаб.1

## Описание 
В рамках лабораторной работы реализован алгоритм умножения матриц на языке C++. Создан класс Matrix, обеспечивающий загрузку, сохранение и перемножение матриц, а также вывод результата в текстовый файл.

Для верификации корректности и анализа производительности разработан вспомогательный модуль на Python. Он автоматически генерирует квадратные матрицы размера n × n со случайными значениями, сохраняет их в файл, запускает исполняемый файл console.exe, а затем сравнивает результат его выполнения с результатом, полученным с помощью метода numpy.dot(). Также производится логирование времени выполнения операций.

Для визуализации полученных данных строится график зависимости времени выполнения (в наносекундах) от размера матрицы.

## Результат
Результирующий график сохраняется в виде изображения .png.
![График](https://github.com/Ryedis/parallel_prog/blob/master/Lab_1/plot.png)

# Отчет лаб.2

## Описание 
В данной работе была реализована многопоточная версия алгоритма умножения матриц с использованием библиотеки OpenMP. Основные изменения коснулись метода dot(), в котором вычисление строк результирующей матрицы осуществляется параллельно с помощью директивы #pragma omp parallel, а ускорение операций суммирования достигается благодаря векторизации: #pragma omp simd reduction(+:sum).

Результаты тестирования показали увеличение производительности при работе с матрицами минимум в 1,3 раз. Так, при использовании 4 потоков ускорение составило в среднем 1,53 раз.

## Результат
![График](https://github.com/Ryedis/parallel_prog/blob/master/Lab_2/plot.png)
![Сравнение 4 потока](https://github.com/Ryedis/parallel_prog/blob/master/Lab_2/plot_delta.png)

# Отчет лаб.3
В данной лабораторной работе была реализована параллельная версия алгоритма умножения квадратных матриц с использованием библиотеки MPI.

## Общий алгоритм

-   Матрица A делится по строкам между всеми доступными процессами.

    Матрица B рассылается (с помощью MPI_Bcast) всем процессам целиком.

    Каждый процесс выполняет умножение своей части матрицы A на B и возвращает полученные строки результата.

    Сбор итоговой матрицы происходит на процессе с рангом 0.

Для равномерного распределения нагрузки учитывается остаток от деления rows % world_size. Взаимодействие между процессами осуществляется с использованием функций MPI_Send, MPI_Recv и MPI_Bcast.

## Результаты

### Выполнение на собственном компьютере.
![График](https://github.com/Ryedis/parallel_prog/blob/master/Lab_3/MPI_test.png)
Алгоритм показывает ожидаемый прирост на 4 потоках, но использование 12-ти не оправдывает затрат мощностей на уменьшение времени рассчета.

### Работа на суперкомпьютере "Сергей Королев"
![График](https://github.com/Ryedis/parallel_prog/blob//master/Lab_3/korolev/korolev_comprasion.png)
На суперкомпьютере алогритм работает лучше, можно сказать, что прирост улучшения линейно связан с потоками.
[Вывод 4 потока](https://github.com/Ryedis/parallel_prog/blob//master/Lab_3/korolev/slurm-124112.out) [Вывод_12 потоков](https://github.com/Ryedis/parallel_prog/blob//master/Lab_3/korolev/slurm-124114.out)